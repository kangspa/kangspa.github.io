---
title: "\"Attention Is All You Need\" - Transformer 논문 리뷰 (1)"
tags:
    - transformer
    - paper
    - 논문
    - 배경
date: "2025-06-17"
thumbnail: "assets/img/DATA & AI/Model/post-04/thumbnail.png"
---
["Attention Is All You Need"](https://arxiv.org/html/1706.03762v7) 논문 내용 리뷰 및 정리한다.
요약, 소개, 제안 배경에 대해 해석 및 간단한 내용을 작성한다.

# Abstract

```markdown
우수한 시퀀스 모델은 인코더와 디코더를 포함한 복합 순환 신경망이나 합성곱 신경망을 기반으로 만들어진다. 또한 최고 성능 모델은 어텐션 매커니즘을 활용해 인코더와 디코더를 연결합니다.
우리는 "트랜스포머"라는 새롭고 간단한 구조를 제안합니다. 이것은 순환 신경망이나 합성곱 신경망을 완전히 배제하고 오직 어텐션 매커니즘만을 기반으로 합니다.
두 개의 기계 번역 작업 실험은 학습에 더 적은 시간을 요구하면서도 병렬 처리가 쉽고 우수한 성능을 보였습니다.
우리 모델은 "WMT 2014 English-to-German" 번역 작업에서 앙상블까지 포함한 기존 모든 기록들보다 2 BLEU 이상 높은 28.4 BLEU 달성했습니다.
"WMT 2014 English-to-French" 번역 작업에서는 41.8이라는 단일 모델 SOTA BLEU를 달성했는데, 3.5일 동안 8개의 GPU를 활용하여 달성한 결과로 최고 모델에 비하면 매우 적은 학습 비용입니다.
우리는 거대하고 제한적인 학습 데이터에서도 영어 번역이 성공적으로 적용되어 트랜스포머가 다른 작업에도 일반화가 잘 됨을 보입니다.
```

Transformer 모델은 기본적으로 회귀 및 합성곱을 일절 사용하지 않고, 순수하게 어텐션 매커니즘만으로 기계어 번역 작업에서 우수한 성능을 보였다.

- `Attention Mechanism`
    무언가 해석할 때, 연관성 높은 중요한 부분을 더욱 참고해서 해석하는 매커니즘.
    입력 시퀀스의 각 부분의 상대적 중요성을 반영하는 어텐션 가중치를 계산하여 적용한다.
    
    - 출처
        - <https://wikidocs.net/22893>
        - <https://www.ibm.com/kr-ko/think/topics/attention-mechanism>
- `BLEU (bilingual evaluation understudy)`
    기계 번역된 텍스트의 품질을 평가하는 알고리즘으로, 기계 출력과 인간 출력 사이의 대응으로 품질을 측정한다.
    명료성이나 문법적 정확성은 고려되지 않으며, 0 ~ 1 사이의 값을 갖는다. 1일 경우 정답과 동일하다는 의미로, 1에 가깝기만 하면 된다.
    
    - 출처
        - <https://ko.wikipedia.org/wiki/BLEU>

# 1 Introduction

```markdown
long short-term memory, gated recurrent 같은 순환 신경망들은 언어 모델 및 기계 번역 같은 시퀀스 모델링 및 변환 문제에서 SOTA로 자리를 굳혔습니다.
회귀 언어 모델과 encoder-decoder 구조의 경계를 넘기 위해 많은 노력이 계속되고 있습니다.

순환 모델은 일반적으로 입력과 출력 시퀀스의 특정 위치에 따른 계산을 처리합니다. 연산 시간의 단계에 따라 위치를 정렬하면서, t에서 입력과 이전 은닉 상태인 h_(t-1)의 함수로 은닉 상태 시퀀스(h_t)를 생성합니다.
이 순차적 처리는 학습 데이터의 병렬 처리를 배제합니다. 이는 배치 제한을 넘어가는 긴 길이에서 치명적으로 작용합니다.
최근 작업에서 조건부 계산과 인수분해를 통해 계산 효율을 크게 향상시켰습니다. 또한 조건부 계산은 모델 성능을 향상시키기도 하였습니다. 하지만 순차 계산이란 근본적 문제가 남아있습니다.

어텐션 메커니즘은 입출력 시퀀스의 거리와 상관없이 의존성을 모델링할 수 있으면서 다양한 작업에서 시퀀스 모델링 및 변환 모델의 중요한 요소가 되었습니다.
일부를 제외한 대부분, 어텐션 매커니즘은 회귀형 네트워크와 결합되어 사용됩니다.

현재 연구에서 우리는 회귀를 피하고, 입출력 사이의 전역 의존성을 도출하기 위해 오직 어텐션 매커니즘에만 의존하는 Transformer를 제안합니다.
Transformer는 병렬화를 많이 허용하고, 8개의 P100 GPU들로 12시간 이내로 학습 후 번역해도 새로운 SOTA를 달성할 수 있습니다.
```

대부분의 어텐션 매커니즘은 순환 모델과 함께 사용되며, 순차적으로 계산을 해야되서 병렬화에 제약이 있었다.
Transformer 모델은 이런 회귀 계산을 사용하지 않고, 순수하게 어텐션 매커니즘만 활용하여 병렬화를 활용하고 높은 학습 효율을 보일 수 있다.

# 2 Background

```markdown
순차 계산 줄이기의 목표는 "Extended Neural GPU", "ByteNet", "ConvS2S" 같은 입출력 위치에 대한 은닉 표현을 병렬 게산하는 모든 합성곱 신경망의 기반이 되었다.
이 모델들에서 임의의 두 입출력 위치 간 신호를 연결하기 위한 연산 횟수는 위치 간 거리에 따라 "ConvS2S"는 선형적으로, "ByteNet"은 로그적으로 증가한다.
이는 먼 위치 사이의 의존성을 학습하기 어렵게 만든다. Transformer는 연산 횟수를 줄이지만, 어텐션 가중치 평균화로 인해 효과적인 해상도가 감소하는 대가가 있습니다.
우리는 3.2에서 설명하는 "Multi-Head Attention"으로 이 효과를 상쇄합니다.

"Intra-attention"으로 불리기도 하는 "Self-attention" 은 단일 시퀀스의 다른 위치를 연관지어 시퀀스 표현을 연산하는 어텐션 매커니즘입니다.
"Self-attention"은 독립적 문장 표현 학습, 문장 함의, 요약, 독해 등 다양한 작업에서 성공적으로 사용되어 왔습니다.

"End-to-end memory networks"는 시퀀스 정렬 회귀 대신 회귀 어텐션 매커니즘에 기반하고, 간단한 언어 질의응답 및 언어 모델 작업에서 좋은 성능을 보여줍니다.

그러나 우리가 아는 한 Transformer는 회귀나 합성곱 없이 입출력 표현을 연산하기 위해 어텐션만 사용한 첫번째 변환 모델입니다.
다음 섹션들에서 Transformer에 대해 설명하고, "self-attention"의 동기를 보이고, [9, 17, 18] 모델에 비해 장점을 논의하겠습니다.
```

Transformer는 효과적인 해상도가 감소하는 단점이 있는데, "Multi-Head Attention"으로 이를 상쇄한다.
"Self-attention"은 다양한 언어 분야에서 효과적으로 사용되어 왔다. Transformer에서 어떻게 사용했는지 보이겠다.

---

트랜스포머 모델을 직접 구축하는 것을 목표로, 논문의 단어 하나하나 최대한 상세하게 읽어가며 리뷰하는 것을 목표로 하고 있다.
다만 생각보다 길어지고, 시간도 오래 걸려서 다른 논문 리뷰는 직역 같은걸 하기보다 간단히 이해한 내용과 중요한 내용 위주로만 정리해야겠다고 생각 중이다.

해당 논문의 제안 배경은, 기존의 어텐션 구조보다 연산이 빠르고, 연산량도 더욱 적은 모델 구조를 만드는 거라고 이해할 수 있다.
이를 위해 핵심이 되는 내용은 "Self-Attention"과 "Multi-Head Attention"으로 보이며, 관련 구조 및 내용은 다음 포스팅에서 작성하겠다.