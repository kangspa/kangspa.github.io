---
title: "트랜스포머 구조"
tags:
   - 구조
   - 어텐션
   - 트랜스포머
date: "2024-11-26"
# thumbnail: "/assets/img/Project/GitBlog/post-05/thumbnail.png"
---

트랜스포머 모델에 대해 알아보기 전에, **'어텐션 메커니즘'**이 무엇인지 알아야한다.

## 어텐션 메커니즘
디코더에서 출력 단어 예측 매 시점마다, 인코더에서 전체 입력 문장을 참고하는 것. 단, 예측할 단어와 연관 있는 입력 단어를 집중(attention)한다.
- 메커니즘 도입 배경
    기존의 RNN에 기반한 seq2seq 는 하나의 고정된 크기의 벡터 표현으로 압축하는데,
    ① 정보 손실 문제
    ② RNN 고질적 문제인 기울기 소실 문제
    로 인해서 고안된 방법이다.
어텐션 함수는 `Attention(Q, K, V) = Attention Value` 로 표현된다.
- Q = Query
    t 시점의 디코더 셀에서의 은닉 상태
- K = Keys
    모든 시점의 인코더 셀의 은닉 상태들
- V = Values
    모든 시점의 인코더 셀의 은닉 상태들
주어진 쿼리에 대해 모든 키와 유사도를 각각 구하고, 이 유사도를 키와 맵핑되어 있는 각각의 값에 반영해준다. 그리고 유사도가 반영된 값을 모두 리턴해주는데, 이를 `Attention Value` 라고 작성해뒀다.
---
## 트랜스포머
seq2seq 구조와 유사하게 인코더-디코더 를 따르지만, 어텐션으로만 구현한 모델
인코더와 디코더가 N개로 구성되는 구조이다. (제안한 논문에서는 각각 6개 사용)

임베딩 벡터들이 입력으로 들어가기 전에 포지셔널 인코딩 값이 더해진다. (순서 정보 보존)
어텐션은 3가지 사용되고, 셀프 어텐션은 2개 있다. (Q, K, V 가 동일한 경우 셀프 어텐션)
-> 동일하다는 것은 벡터의 출처가 같다.
- 인코더의 셀프 어텐션
    Query = Key = Value
- 디코더의 마스크드 셀프 어텐션
    Query = Key = Value
- 디코더의 인코더-디코더 어텐션
    Query : 디코더 벡터
    Key = Value : 인코더 벡터
**셀프 어텐션**은 자기 주체적으로 각 단어 간 관계를 파악하는데 도움이 된다.

### 장점
1. 어텐션을 통해 병렬 프로세싱에 적합한 연산이라 모델의 실행 속도가 빨라진다.
2. 순서 정보를 보존함으로써 문맥 이해력을 높일 수 있다.
3. 모델 크기를 빠르게 확장시킬 수 있다.

### 단점
1. 어텐션 연산 과정이 막대한 연산량을 필요로 한다 -> 메모리 사용량 증가
2. 입력 시퀀스 길이가 길수록 장거리 의존성 문제 -> 멀리 떨어진 토큰 간 관계 파악이 어려워짐
---
- 참고사항
  - https://modulabs.co.kr/blog/beyond-transformer/
  - https://www.ibm.com/kr-ko/topics/transformer-model
  - https://wikidocs.net/31379
  - https://blogs.nvidia.co.kr/blog/what-is-a-transformer-model/