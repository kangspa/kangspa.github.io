---
title: "과적합과 방지방법"
tags:
   - 과적합
   - Lasso
   - Ridge
date: "2024-11-25"
# thumbnail: "/assets/img/Project/GitBlog/post-05/thumbnail.png"
---

## 과적합

모델, 알고리즘이 학습 데이터에 지나치게 적합한 상태가 되어, 해당 데이터셋이 아닌 다른 데이터에서는 정확한 예측, 추론을 하지 못하는 현상.
즉, **일반화**되지 않은 상황을 말한다.

- 참고 : 과소 적합
    모델이 충분히 학습하지 못해서 입력과 출력 간 의미 있는 관계를 결정하지 못하는 상태

__과적합, 과소적합 방지를 위해 검증 데이터셋에 대한 성능과 비교하며 적합한 학습 횟수를 선정해야한다.__

# 방지하는 방법

## 데이터 측면

1. 데이터 수집
    더욱 많은 데이터를 구해와서 학습에 사용한다.

2. 데이터 증강
    노이즈나 방향, 색깔 등 기존 데이터에 다양한 영상 처리기법을 추가해 데이터 양을 늘려준다. (이미지)

3. 데이터 내 특징 선택
    해결하는 목적과 관련성 없거나 중복되는 특징을 제거함으로써 학습 효율을 높여준다.

## 모델 측면

1. 모델 복잡도 줄이기
    은닉층의 수나 매개변수의 수 등을 줄임으로써 한 번의 학습이 현재 데이터에 과적합되는 것을 방지해준다.

2. 정규화 (가중치 규제)
    - L1 (Lasso) 규제
        가중치 w들의 절대값 합을 비용 함수에 추가하는 것.
        일부 가중치를 완전히 0으로 축소하고 제거하는 특성이 있다.
    - L2 (Ridge) 규제
        모든 가중치 w들의 제곱합을 비용 함수에 추가하는 것.
        가중치를 0에 가깝게 만들지만 완전히 0으로 만들지는 않는다.
    - 규제 강도가 강할수록 좀 더 일반화된다. (더 큰 입력 매개변수에 '페널티'를 적용하여 모델의 분산량을 제한)

3. 드롭아웃
    학습 시 신경망의 일부를 비활성화하여, 사용하지 않는 방법이다.
    한번의 학습 진행 시 일부 신경망 비활성화를 하고, 이후에는 또 다른 신경망을 비활성화 하는 등 모델 내부의 신경망들이 서로 다른 데이터로 학습한 것과 비슷한 효과를 보게 해준다.
    이를 통해 앙상블과 같은 효과를 볼 수 있다.

- 출처
  - https://wikidocs.net/61374
  - https://www.ibm.com/kr-ko/topics/overfitting
  - https://iotnbigdata.tistory.com/15